CREATE TABLE source_table(
  id                         DECIMAL(20, 0),
  company_id                 BIGINT,
  shareholder_id             STRING,
  shareholder_entity_type    SMALLINT,
  shareholder_name_id        BIGINT,
  investment_ratio_total     DECIMAL(24, 12),
  is_controller              SMALLINT,
  is_ultimate                SMALLINT,
  is_big_shareholder         SMALLINT,
  is_controlling_shareholder SMALLINT,
  equity_holding_path        STRING,
  create_time                TIMESTAMP(3),
  update_time                TIMESTAMP(3),
  is_deleted                 SMALLINT,
  op_ts as CAST(CURRENT_TIMESTAMP AS TIMESTAMP(3)),
  PRIMARY KEY (id) NOT ENFORCED
) WITH (
  'connector' = 'jdbc',
  'url' = 'jdbc:mysql://10.99.131.246:3306/company_base',
  'table-name' = 'ratio_path_company',
  'username' = 'jdhw_d_data_dml',
  'password' = '2s0^tFa4SLrp72',
  'scan.partition.column' = 'id',
  'scan.partition.num' = '40000',
  'scan.partition.lower-bound' = '1',
  'scan.partition.upper-bound' = '400000000',
  'scan.fetch-size' = '1024'
);
create table ratio_path_company(
  id                         DECIMAL(20, 0),
  company_id                 BIGINT,
  shareholder_id             STRING,
  shareholder_entity_type    SMALLINT,
  shareholder_name_id        BIGINT,
  investment_ratio_total     DECIMAL(24, 12),
  is_controller              SMALLINT,
  is_ultimate                SMALLINT,
  is_big_shareholder         SMALLINT,
  is_controlling_shareholder SMALLINT,
  equity_holding_path        STRING,
  create_time                TIMESTAMP(3),
  update_time                TIMESTAMP(3),
  is_deleted                 SMALLINT,
  op_ts                      TIMESTAMP(3),
  PRIMARY KEY (id) NOT ENFORCED
) WITH (
  'connector' = 'hudi',
  'path' = 'obs://hadoop-obs/hudi_ods/ratio_path_company007',
  'table.type' = 'MERGE_ON_READ',
  -- cdc
  'changelog.enabled' = 'true',
  -- index
  'index.type' = 'BUCKET',
  'hoodie.bucket.index.num.buckets' = '128',
  -- write
  'write.operation' = 'bulk_insert',
  'write.bulk_insert.shuffle_input' = 'false',
  'write.bulk_insert.sort_input' = 'false',
  'write.tasks' = '64',
  'write.precombine' = 'true',
  'write.precombine.field' = 'op_ts',
  -- compaction
  'compaction.schedule.enabled' = 'false',
  'compaction.async.enabled' = 'false',
  -- clean
  'clean.async.enabled' = 'false'
);
insert into ratio_path_company select * from source_table;


#!/bin/bash
####################################
#                                  #
#             variable             #
#                                  #
####################################
# 类名
export className=DemoJob
# 配置文件名 config.yml or repair.yml
export configName=hudi.yml

####################################
#                                  #
#              ignore              #
#                                  #
####################################
# flink env
export FLINK_HOME=/home/hive/hudi/flink-1.15.4
export FLINK_CONF_DIR=/home/hive/hudi/flink-conf
export HADOOP_CLASSPATH=`hadoop classpath`
# checkpoint dir
export folderName=$(echo ${className} | sed -E 's/([A-Z])/-\1/g' | sed -E 's/^-//g' | tr 'A-Z' 'a-z')
# yarn application name
export jobName=$(if [[ ${configName} == config* ]] ; then echo ${className} ; else echo ${className}RepairTest ; fi)
# restore dir
export restoreDir=$(if [[ $1 == hdfs* ]] ; then echo '-s '$1 ; else echo '' ; fi)

####################################
#                                  #
#    maybe change Memory or Slot   #
#                                  #
####################################
/home/hive/hudi/flink-1.15.4/bin/flink run-application -t yarn-application ${restoreDir} \
  -D jobmanager.memory.process.size=1g \
  -D taskmanager.memory.process.size=4g \
  -D taskmanager.numberOfTaskSlots=8 \
  -D parallelism.default=64 \
  -D state.checkpoints.dir=hdfs:///hudi/flink-checkpoints/${folderName} \
  -D yarn.application.priority=100 \
  -D yarn.ship-files=${configName} \
  -D yarn.application.name=HudiTest \
  -c com.liang.hudi.job.${className} hudi-1.0.jar ${configName}
