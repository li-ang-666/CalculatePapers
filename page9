CREATE TABLE source_table (
  id                DECIMAL(20,0),
  graph_id          BIGINT,
  base              STRING,
  name              STRING,
  legal_person_id   BIGINT,
  legal_person_name STRING,
  legal_person_type INT,
  code              STRING,
  reg_number        STRING,
  company_org_type  STRING,
  reg_location      STRING,
  establish_date    DATE,
  from_date         DATE,
  to_date           DATE,
  business_scope    STRING,
  reg_institute     STRING,
  approved_date     DATE,
  reg_status        STRING,
  reg_capital       STRING,
  org_number        STRING,
  source_flag       STRING,
  crawled_time      TIMESTAMP(3),
  deleted           TINYINT,
  create_time       TIMESTAMP(3),
  update_time       TIMESTAMP(3),
  op_ts TIMESTAMP(3) METADATA FROM 'value.ingestion-timestamp' VIRTUAL,
  PRIMARY KEY (id) NOT ENFORCED
) WITH (
  'connector' = 'kafka',
  'topic' = '56736.json.prism.enterprise',
  'properties.bootstrap.servers' = '10.99.202.90:9092,10.99.206.80:9092,10.99.199.2:9092',
  'properties.group.id' = 'hudi-demo-job',
  'scan.startup.mode' = 'earliest-offset',
  -- canal
  'format' = 'canal-json',
  'canal-json.ignore-parse-errors' = 'true',
  'canal-json.encode.decimal-as-plain-number' = 'true'
);
create table enterprise(
  id                DECIMAL(20,0),
  graph_id          BIGINT,
  base              STRING,
  name              STRING,
  legal_person_id   BIGINT,
  legal_person_name STRING,
  legal_person_type INT,
  code              STRING,
  reg_number        STRING,
  company_org_type  STRING,
  reg_location      STRING,
  establish_date    DATE,
  from_date         DATE,
  to_date           DATE,
  business_scope    STRING,
  reg_institute     STRING,
  approved_date     DATE,
  reg_status        STRING,
  reg_capital       STRING,
  org_number        STRING,
  source_flag       STRING,
  crawled_time      TIMESTAMP(3),
  deleted           TINYINT,
  create_time       TIMESTAMP(3),
  update_time       TIMESTAMP(3),
  op_ts             TIMESTAMP(3),
  PRIMARY KEY (id) NOT ENFORCED
) WITH (
  'connector' = 'hudi',
  'path' = 'obs://hadoop-obs/hudi_ods/enterprise001',
  'table.type' = 'MERGE_ON_READ',
  -- cdc
  'changelog.enabled' = 'true',
  -- index
  'index.type' = 'BUCKET',
  'hoodie.bucket.index.num.buckets' = '128',
  -- write
  'write.tasks' = '4',
  'write.task.max.size' = '512',
  'write.batch.size' = '8',
  'write.log_block.size' = '64',
  'write.precombine' = 'true',
  'write.precombine.field' = 'op_ts',
  -- compaction
  'compaction.schedule.enabled' = 'true',
  'compaction.async.enabled' = 'true',
  'compaction.delta_commits' = '30',
  -- clean
  'clean.async.enabled' = 'true',
  'clean.retain_commits' = '2880'
);
insert into enterprise select * from source_table;